date,arxiv_num,title,authors,categories,comments,abstract,networked,multi agent systems,reinforcement learning ,keyword count
arXiv:2507.16306,\\,COMPASS: Cooperative Multi-Agent Persistent Monitoring using  Spatio-Temporal Attention Network,"Xingjian Zhang, Yizhuo Wang, Guillaume Sartoretti",cs.MA cs.RO,0,"  Persistent monitoring of dynamic targets is essential in real-world applications such as disaster response, environmental sensing, and wildlife conservation, where mobile agents must continuously gather information under uncertainty. We propose COMPASS, a multi-agent reinforcement learning (MARL) framework that enables decentralized agents to persistently monitor multiple moving targets efficiently. We model the environment as a graph, where nodes represent spatial locations and edges capture topological proximity, allowi= ng agents to reason over structured layouts and revisit informative regions as needed. Each agent independently selects actions based on a shared spatio-temporal attention network that we design to integrate historical observations and spatial context. We model target dynamics using Gaussian Processes (GPs), which support principled belief updates and enable uncertainty-aware planning. We train COMPASS using centralized value estima= tion and decentralized policy execution under an adaptive reward setting. Our extensive experiments demonstrate that COMPASS consistently outperforms str= ong baselines in uncertainty reduction, target coverage, and coordination efficiency across dynamic multi-target scenarios. ",0.0,0.0,1.0,1
arXiv:2507.16479,\\,Arbitrage Tactics in the Local Markets via Hierarchical Multi-agent  Reinforcement Learning,"Haoyang Zhang, Mina Montazeri, Philipp Heer, Koen Kok, Nikolaos G.  Paterakis",eess.SY cs.SY,0,"  Strategic bidding tactics employed by prosumers in local markets, includi= ng the Local Electricity Market (LEM) and Local Flexibility Market (LFM), have attracted significant attention due to their potential to enhance economic benefits for market participants through optimized energy management and bidding. While existing research has explored strategic bidding in a single market with multi-agent reinforcement learning (MARL) algorithms, arbitrage opportunities across local markets remain unexplored. This paper introduces=  a hierarchical MARL (HMARL) algorithm designed to enable aggregator arbitrage across multiple local markets. The strategic behavior of these aggregators = in local markets is modeled as a two-stage Markov game: the first stage involv= es the LEM, while the second stage encompasses both the LFM and the balancing market. To solve this two-stage Markov game, the HMARL framework assigns two sub-agents to each aggregator, a primary sub-agent and a secondary sub-agen= t. Without the arbitrage strategy, these sub-agents operate in silos, with the primary sub-agent focusing on first-stage profits and the secondary sub-age= nt on second-stage profits, each employing independent MARLs. On the contrary, when implementing the arbitrage strategy with the proposed HMARL, the sub-agents communicate and coordinate to perform arbitrage across multiple local markets, enhancing overall efficiency. The case study, conducted unde= r a scenario where all aggregators employ the arbitrage strategy, shows that despite higher initial costs in the LEM, this strategy generates substantial savings in the LFM and the balancing market, resulting in a total profit increase of $40.6\%$ on average. This highlights the capability of the prop= osed HMARL to address the two-stage Markov game and facilitate arbitrage across local markets, thereby enhancing profitability for participants. ",0.0,0.0,1.0,1
arXiv:2507.16520,\\,A Distributed Actor-Critic Algorithm for Fixed-Time Consensus in  Nonlinear Multi-Agent Systems,Aria Delshad and Maryam Babazadeh,eess.SY cs.SY,0,"  This paper proposes a reinforcement learning (RL)-based backstepping cont= rol strategy to achieve fixed time consensus in nonlinear multi-agent systems w= ith strict feedback dynamics. Agents exchange only output information with their neighbors over a directed communication graph, without requiring full state measurements or symmetric communication. Achieving fixed time consensus, wh= ere convergence occurs within a pre-specified time bound that is independent of initial conditions is faced with significant challenges due to the presence=  of unknown nonlinearities, inter-agent couplings, and external disturbances. T= his work addresses these challenges by integrating actor critic reinforcement learning with a novel fixed time adaptation mechanism. Each agent employs an actor critic architecture supported by two estimator networks designed to handle system uncertainties and unknown perturbations. The adaptation laws = are developed to ensure that all agents track the leader within a fixed time regardless of their initial conditions. The consensus and tracking errors a= re guaranteed to converge to a small neighborhood of the origin, with the convergence radius adjustable through control parameters. Simulation results demonstrate the effectiveness of the proposed approach and highlight its advantages over state-of-the-art methods in terms of convergence speed and robustness. ",0.0,0.0,1.0,1
arXiv:2507.16249 (*cross-list,\\,Multi-Agent Reinforcement Learning for Sample-Efficient Deep Neural  Network Mapping,"Srivatsan Krishnan, Jason Jabbour, Dan Zhang, Natasha Jaques,  Aleksandra Faust, Shayegan Omidshafiei, Vijay Janapa Reddi",cs.LG cs.MA,0,"  Mapping deep neural networks (DNNs) to hardware is critical for optimizing latency, energy consumption, and resource utilization, making it a cornerst= one of high-performance accelerator design. Due to the vast and complex mapping space, reinforcement learning (RL) has emerged as a promising approach-but = its effectiveness is often limited by sample inefficiency. We present a decentralized multi-agent reinforcement learning (MARL) framework designed = to overcome this challenge. By distributing the search across multiple agents,=  our framework accelerates exploration. To avoid inefficiencies from training multiple agents in parallel, we introduce an agent clustering algorithm that assigns similar mapping parameters to the same agents based on correlation analysis. This enables a decentralized, parallelized learning process that significantly improves sample efficiency. Experimental results show our MARL approach improves sample efficiency by 30-300x over standard single-agent R= L, achieving up to 32.61x latency reduction and 16.45x energy-delay product (E= DP) reduction under iso-sample conditions. ",0.0,0.0,2.0,1
arXiv:2507.16481 (*cross-list,\\,Guided Reinforcement Learning for Omnidirectional 3D Jumping in  Quadruped Robots,"Riccardo Bussola, Michele Focchi, Giulio Turrisi, Claudio Semini,  Luigi Palopoli",cs.RO cs.SY eess.SY,0,"  Jumping poses a significant challenge for quadruped robots, despite being crucial for many operational scenarios. While optimisation methods exist for controlling such motions, they are often time-consuming and demand extensive knowledge of robot and terrain parameters, making them less robust in real-world scenarios. Reinforcement learning (RL) is emerging as a viable alternative, yet conventional end-to-end approaches lack efficiency in term= s of sample complexity, requiring extensive training in simulations, and predictability of the final motion, which makes it difficult to certify the safety of the final motion. To overcome these limitations, this paper introduces a novel guided reinforcement learning approach that leverages physical intuition for efficient and explainable jumping, by combining B\'e= zier curves with a Uniformly Accelerated Rectilinear Motion (UARM) model. Extens= ive simulation and experimental results clearly demonstrate the advantages of o= ur approach over existing alternatives. ",0.0,0.0,2.0,1
arXiv:2504.06439,\\,Graph Neural Network-Based Distributed Optimal Control for Linear  Networked Systems: An Online Distributed Training Approach,"Zihao Song, Shirantha Welikala, Panos J. Antsaklis and Hai Lin",eess.SY cs.LG cs.SY,"9 pages, 4 figures ","\\ ( <a href=3D""https://urldefense.us/v3/__https://arxiv.org/abs/2504.06439= __;!!G_uCfscf7eWS!ciUKqh37PUNTjAxN3IgSMcIZofZl-78dBbb_IzfB2igJnFZVOmQiYyAie= HcR3srYmg5qPLLGIwrVGLwv1L0$"">https://urldefense.us/v3/__https://arxiv.org/a= bs/2504.06439__;!!G_uCfscf7eWS!ciUKqh37PUNTjAxN3IgSMcIZofZl-78dBbb_IzfB2igJ= nFZVOmQiYyAieHcR3srYmg5qPLLGIwrVGLwv1L0$</a> ,  941kb) ---------------------------------------------------------------------------=  ",1.0,0.0,0.0,1
